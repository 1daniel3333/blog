import streamlit as st
from langchain_ollama.llms import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
# from config import CHAT_PROMPT_TEMPLATE
# Title on the page

def llama_talk():
    st.markdown(
        "<h2 style='text-align: center; color: #4CAF50; font-family: Arial;'>Dan's AI assistantğŸª¶</h2>",
        unsafe_allow_html=True,
    )

    CHAT_PROMPT_TEMPLATE = """
    As a headhunter, you've collect information about your candidate and ready introduce to your boss. 
    Your boss will ask you question about the candidate.
    You will provide responses to questions related to the candidate and politely avoid answering unrelated questions.
    Candidate information below:
    ---Start---
    My name is Dan Iâ€™m semiconductor failure data analytics (DA) and full stack engineer proficient in python and SQL with over 8-year experience. Our tool had on average 80 click per day and saving the equivalent of 16 headcounts annually. My unique value lies in using semiconductor domain and data-driven analysis skill to resolve tasks, whether itâ€™s customer escalations, defects finding, automation pipeline or priority arrangement. I have collaborated with software and data engineers to create robust pipelines and communicated effectively with stakeholders to identify and deliver business value. Iâ€™m committed to continuous learning and staying ahead of Industry and programming trend. With my material science background, I know about semiconductor languages. And I learn coding source during night to increase my coding skills. 2024 is Full of LLM and Machine Learning, I also learn many courses online and applied to work. Passionate about coding, with a top 10% ranking on LeetCode. Additionally, Iâ€™ve earned 32 certifications from Coursera and other platforms.
    I learn these knowleade from online course, books and from my own research. 
    My complete courses is   
    â€¢ Advanced Learning Algorithms
    â€¢ Generative AI with Large Language Models
    â€¢ Supervised Machine Learning: Regression and Classification
    â€¢ è€åŒ–å…¨æ–¹ä½æ‡‰å°æ‰‹å†Š
    â€¢ Test-Driven Development Overview
    â€¢ Clean Code
    â€¢ SQL for Data Science
    â€¢ Foundations of Project Management
    â€¢ Generative AI for Everyone
    â€¢ Crash Course on Python
    â€¢ AI For Everyone
    â€¢ Build a Machine Learning Web App with Streamlit and Python
    â€¢ Introduction to Github and Visual Studio Code
    â€¢ Foundations of User Experience (UX) Design
    â€¢ Learning How to Learn: Powerful mental tools to help you master tough subjects
    â€¢ Running Distributed TensorFlow using Vertex AI
    â€¢ Google Data Analytics
    â€¢ Google Cloud Big Data and Machine Learning Fundamentals
    â€¢ How Google does Machine Learning
    â€¢ Programming for Everybody
    â€¢ R Programming A-Zâ„¢: R For Data Science With Real Exercises!
    â€¢ PythonåŸºç¤èª²ç¨‹å’Œç¶²è·¯çˆ¬èŸ²å…¥é–€å¯¦æˆ°
    â€¢ The Complete SQL Bootcamp: Go from Zero to Hero
    â€¢ Tableau Advanced: Master Tableau in Data Science
    â€¢ JMP Training for Statistics & Data Visualization
    â€¢ Tableau A-Z: Hands-On Tableau Training for Data Science
    â€¢ HiSKIOï¼šæ‰“æ»¾ä¹Ÿè¦æœ‰æŠ€å·§ï¼ŒNickçš„å°ˆæ¥­æˆé•·å¯¦è¸èª²
    ---End---
    Boss: {question}
    Assistant: """

    template = CHAT_PROMPT_TEMPLATE
    prompt = ChatPromptTemplate.from_template(template)

    #Load the local Llama3.2 model that we pulled using ollama
    model = OllamaLLM(model="llama3.2")
    chain = prompt | model

    #Initialize message history in session state
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "Hi! How may I help you?"}
        ]

    #Display the chat history
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Handle user input
    if user_input := st.chat_input("What is up?"):
        # Add user message to session state
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        # Generate assistant response
        with st.chat_message("assistant"):
            response = chain.invoke({"question": user_input})
            st.markdown(response)

        # Add assistant response to session state
        st.session_state.messages.append({"role": "assistant", "content": response})